{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ace34f-c314-442e-89f4-c9f2485ee5b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d0b5f0-c223-4159-90c2-a99ff103c04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Optimizing construction schedules using large language models (LLMs) involves leveraging advanced AI capabilities to analyze, understand, and optimize various aspects of construction project management. \n",
    "\n",
    "Step 1: Data Collection and Preparation\n",
    "\n",
    "First, gather and prepare the data needed for construction schedule optimisation. This may include task details, predecessor relationships, \n",
    "resource allocations, and other relevant information.\n",
    "\n",
    "Step 2: Define the Knowledge Graph\n",
    "\n",
    "Create a knowledge graph to represent the construction schedule, including tasks, predecessor tasks, resources, \n",
    "and their relationships. You can use libraries like networkx for this purpose.\n",
    "\n",
    "Step 3: Integrate a Large Language Model\n",
    "\n",
    "Integrate an LLM like OpenAI's GPT-4 or similar to analyze and optimize the construction schedule. \n",
    "Use libraries like transformers from Hugging Face for this purpose.\n",
    "\n",
    "Step 4: Optimization Logic\n",
    "\n",
    "Develop the logic to optimize the construction schedule. This may include:\n",
    "\n",
    "    Analyzing task dependencies\n",
    "    Allocating resources efficiently\n",
    "    Predicting potential delays and mitigating risks\n",
    "\n",
    "Step 5: Implementation in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2b3a98e-a6b4-42d8-9f1e-3f461fea3165",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f723541fa52d47078029e7ee95281332",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "684054108d954511af2c625449242ca2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d92e2248673a421dbe4da3b8c28ffee1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "481d24ddaa474725834db3195eb21b9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8580e55989a4e1a833b0c783db68f07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d16fe81815164c3abe3379ddb692b67f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c2fae8d0c6440509e0569230a2ed6e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import networkx as nx\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Initialise the language model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a127597-61db-45b6-9e07-c1f3fe437ac7",
   "metadata": {},
   "source": [
    "### Define KG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4594d00f-23b7-41fe-a0e7-b0a211e4688a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the graph\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# Add nodes for Tasks, PredTasks, TasksRSRC, and RSRC\n",
    "G.add_node(\"Task1\", type=\"Task\", duration=5)\n",
    "G.add_node(\"Task2\", type=\"Task\", duration=3)\n",
    "G.add_node(\"Resource1\", type=\"RSRC\")\n",
    "G.add_node(\"Resource2\", type=\"RSRC\")\n",
    "G.add_node(\"Task1-Resource1\", type=\"TasksRSRC\")\n",
    "G.add_node(\"Task2-Resource2\", type=\"TasksRSRC\")\n",
    "\n",
    "# Add edges representing relationships\n",
    "G.add_edge(\"Task1\", \"Task2\", type=\"HAS_PREDTASK\")\n",
    "G.add_edge(\"Task1\", \"Resource1\", type=\"REQUIRES_RSRC\")\n",
    "G.add_edge(\"Task2\", \"Resource2\", type=\"REQUIRES_RSRC\")\n",
    "G.add_edge(\"Resource1\", \"Task1-Resource1\", type=\"ALLOCATED_TO\")\n",
    "G.add_edge(\"Resource2\", \"Task2-Resource2\", type=\"ALLOCATED_TO\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ca5d5a-6adf-40ba-8f46-8f4abb37a42f",
   "metadata": {},
   "source": [
    "### Define Optimisation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a635a8a-9148-457e-acd9-0b85a1c5691a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_schedule(graph, model, tokenizer):\n",
    "    # Convert the knowledge graph to a textual representation\n",
    "    text_input = \"Optimise the following construction schedule:\\n\"\n",
    "    for node in graph.nodes(data=True):\n",
    "        text_input += f\"{node[0]}: {node[1]}\\n\"\n",
    "    for edge in graph.edges(data=True):\n",
    "        text_input += f\"{edge[0]} -> {edge[1]}: {edge[2]}\\n\"\n",
    "\n",
    "    # Use the language model to optimise the schedule\n",
    "    inputs = tokenizer(text_input, return_tensors=\"pt\")\n",
    "    outputs = model.generate(inputs[\"input_ids\"], max_length=512)\n",
    "    optimized_schedule = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    return optimized_schedule\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f98b4b3-a881-433c-9206-afae77536ec1",
   "metadata": {},
   "source": [
    "### Run the optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab65609b-e51d-450b-8c7f-2d356c4121cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ": Task1 -> Task1: 'type': 'Task', 'duration': 5 Task2: 'type': 'HAS_PREDTASK' Task1 -> Resource1: 'type': 'REQUIRES_RSRC' Task1 -> Resource1: 'type': 'ALLOCATED_TO' Resource2 -> Task2-Resource2: 'type':\n"
     ]
    }
   ],
   "source": [
    "optimized_schedule = optimize_schedule(G, model, tokenizer)\n",
    "print(optimized_schedule)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929f08ef-08b9-4a50-b104-0a7c65c78925",
   "metadata": {},
   "source": [
    "Explanation\n",
    "\n",
    "    Data Collection and Preparation: Gather all necessary data related to tasks, resources, and their relationships.\n",
    "    Knowledge Graph: Use networkx to create a knowledge graph representing tasks, predecessors, and resource allocations.\n",
    "    Large Language Model: Use a pre-trained model like T5 from Hugging Face to analyze and optimise the schedule.\n",
    "    Optimisation Logic: Convert the knowledge graph to a textual representation and use the LLM to generate an optimised schedule.\n",
    "    Implementation: The implementation includes defining nodes and edges in the graph, creating the optimisation function, and running the optimisation.\n",
    "\n",
    "This approach provides a structured way to use LLMs for optimising construction schedules by integrating data-driven insights with advanced AI capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d634c98e-71e6-41b3-8f8c-21524a634e8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
